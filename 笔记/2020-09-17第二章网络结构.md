# 第二课 Network Structure
## 回顾
* 大脑有大量的神经元组成，$10^{11}$个神经元
* 神经网络存在分层结构，比如视网膜由三层神经元构成
  * 下一层的神经元以上一层的输出信号作为输入
  *  **新皮层（neocortex）**
     * $1000cm^2$大小
     * 只有2mm厚
     * 300亿（几乎1/3的神经元）集中于此
     * 被划分成一个个的功能区，以层级方式相互连接（Concept of hierarchy)
## 神经元的可计算模型（Computational Model of Neurons）
* 通过数学化，模式化的模型模拟神经元
* 是一种数学公式
### 抽象化
* 抽象神经元之间的连接，进行拓扑抽象（长啥样就画啥样）
  * 联结（Synapse）
    * 用数值刻画两个神经元的联结强度（连接权，$w$)
      * w > 0, 兴奋性联结
      * w = 0，不具备联结
      * w < 0，抑制性联结 
* 数学模型：$a=f(\sum^n_{i=1}w_ix_i)$
* $f(x)$称为 **激活公式**
* 对于每一个胞体，，对于输入x和权重w进行处理，得到一个total input $z=\sum^n_{i=1}w_ix_i$，并利用胞体的处理 $f$(Actication function，结合函数)得到该胞体实际输出的 a
  * 结合函数的选择
    * 线性函数（linear Function）
    * (Rectifier function)
    * (Hard-limit function)
    * (sigmoid function)
    * 正弦(Sine function)
## 神经网络的可计算模型(数学模型，神经网络的标准模型)
* 信息在神经网络内以“前向流动”进行,层与层之间进行**前馈计算**
* ***静态观点：*** 从第一个向量到最后一个向量，一层一层之间的高度迭代的**非线性映射**
* ***动态观点：*** Dynamical system，离散动力学系统，随着时间推进不断变化
### 重要特征
* 没有同层之间的两个神经元的链接
* 没有不是相邻层之间的神经元的链接
### 构成方式
1. 最基本的结构
   1. 即上一节中每一个神经元的数学模型
2. 每一层的结构
    * 一层中的n个神经元，根据输出不同，组成一个n维向量
3. 层与层之间的连接
   1. 每一个连线用连接权表示
   2. 有两个下标，比如前一层的$a^l_j$神经元和下一层的$a^{l+1}_i$个神经元的连接记为$w^l_{ij}$ 
   3. 两层之间所有的连接权，可以排列组成一个连接矩阵（连接权矩阵）
4. 总体结构的所有元素
   1. 层数示例：$L$
   2. 神经元个数：$n_i$
   3. 连接权矩阵：$W_i$
   4. 输出向量：$a_i$

## 多种神经网络模型
* 标准神经网络（上一章中所介绍的神经网络）
1. 具有**外部输入（External inputs）** 的神经网络模型
   1. 定义：非初始层中，存在未与上一层连接的神经元，向网络中输入外部信号
   2. 本质上变化并不大
2. CNNs：Share Connections Weights Between Layers
   1. 卷积神经网络（convolutional neural networks)，实现了共享连接机制。
   2. 定义：以一组固定的连接权，通过一起位移计算下一层的所有元素。
3. RNNs:Share connection weights in all layers
   1. 回复神经网络、递归神经网络（recurrent Neural networks）
   2. $n_1=n_2=...=n_l=n$ <br> $W^1=W^2=...=W^L=W$
   3. 其实只需要一层神经元，将输出返回，作为输入即可（递归操作），由一个时刻计算下一个时刻
   4. 层的概念换成时间，可以称作 **“离散神经网络”**
   5.  $$a^{l+1}=f(Wa^l)$$
4. CRNNs：Continuous recurrent neural networks
   1. 连续神经网络，通过微分的思想实现
$$\frac{da_i(t)}{dt}=-a_i(t)+f \left( \sum^n_{j=1}w_{ij}a_j(t)\right)$$